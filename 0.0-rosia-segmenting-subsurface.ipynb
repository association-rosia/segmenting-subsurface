{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Every Layer, Everywhere, All at Once: Segmenting Subsurface\n",
    "\n",
    "<img src='assets/banner.png'>\n",
    "\n",
    "## üåã Context\n",
    "\n",
    "Seismic data serves as a window into the Earth, aiding Geophysicists in identifying and mapping rock layers and structures, akin to how radiologists use MRI data for medical diagnoses. Its applications include reservoir identification, CO2 sequestration monitoring, oil and gas exploration, and groundwater management. However, interpreting seismic data presents challenges due to its geological complexity, including ambiguous features like faults and pinch outs.\n",
    "\n",
    "The challenge provides around 9,000 pre-interpreted seismic volumes with segment masks for model training, each containing unique geological features. Participants are encouraged to use SAM (Seismic Attribute Mapping) in their 3D data segmentation pipelines. Solutions can involve tools like UNET for segment generation, feeding into the SAM model for refinement. The output should be a NumPy array with segment IDs for labeled intervals. Experimentation is encouraged with the provided training data split into four tranches for accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Libraries\n",
    "\n",
    "Our code run on `Python 3.10.13`\n",
    "\n",
    "Installing external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing segmenting-subsurface librarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e .\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as tvF\n",
    "\n",
    "# Custom package (accessible into src directory)\n",
    "from src import utils\n",
    "import src.visualization.utils as vutils\n",
    "import src.data.make_dataset as md\n",
    "import src.models.segformer.train_model as segformer_tm\n",
    "import src.models.mask2former.train_model as mask2former_tm\n",
    "import src.features.segment_anything_inference as sam_inf\n",
    "import src.features.segformer_inference as segformer_inf\n",
    "import src.features.mask2former_inference as mask2former_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to wandb RosIA for demonstration account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Initialisation \n",
    "wandb_api_key = '02ca932e1203e93aaa8c97b8331d6c0b04c3170a' # Please do not share this api key\n",
    "! wandb login --relogin {wandb_api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∏ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_path = 'data/raw/train/69764103/seismic_block-2_vol_69764103.npy'\n",
    "volume = np.load(volume_path, allow_pickle=True)\n",
    "\n",
    "label_path = volume_path.replace('seismic', 'horizon_labels')\n",
    "label = np.load(label_path, allow_pickle=True)\n",
    "\n",
    "volume_hollow = vutils.get_volume_hollow(volume)\n",
    "volume_plotly = vutils.get_plotly_volume(volume_hollow, colorscale='Greys')\n",
    "\n",
    "label_hollow = vutils.get_volume_hollow(label)\n",
    "label_plotly = vutils.get_plotly_volume(label_hollow, colorscale='Viridis')\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[\n",
    "        [{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}]\n",
    "    ],\n",
    "    subplot_titles=(\"Original volume\",\"Labelised volume\")\n",
    ")\n",
    "fig.add_trace(volume_plotly, row=1, col=1)\n",
    "fig.add_trace(label_plotly, row=1, col=2)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "del label_hollow, volume_hollow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öíÔ∏è Preprocessing\n",
    "\n",
    "In computer vision, the classic preprocessing steps for an image are as follows:\n",
    "\n",
    "1. `Scaling`: Allows us to scale the values between 0 and 1. (Using a Min Max Scaler)\n",
    "\n",
    "2. `Normalization`: Helps us achieve a Gaussian distribution of values for each channel. (Using a Standard Scaler)\n",
    "\n",
    "3. `Rescaling`: If necessary, based on what input the model accepts. (Using a bilinear interpolation)\n",
    "\n",
    "However, we have observed that in the images of our dataset, the objective is to delineate areas of varying brightness between them. That's why we decided to add `contrast` to highlight these differences in shade between the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config()\n",
    "\n",
    "def scale(image):\n",
    "    image = (image - config['data']['min']) / (config['data']['max'] - config['data']['min'])\n",
    "    return image\n",
    "\n",
    "def contrast(image):\n",
    "    tensor = torch.from_numpy(image).unsqueeze(0)\n",
    "    tensor = tvF.adjust_contrast(tensor, contrast_factor=25)\n",
    "    \n",
    "    return tensor.squeeze().numpy(force=True)\n",
    "\n",
    "def normalize(image):\n",
    "    tensor = torch.from_numpy(image).unsqueeze(0)\n",
    "    tensor = tvF.normalize(tensor, mean=[config['data']['mean']], std=[config['data']['std']])\n",
    "    \n",
    "    return tensor.squeeze().numpy(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idx = 200\n",
    "image = volume[slice_idx, :, :].T\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}, {\"type\": \"heatmap\"}],\n",
    "        [{\"type\": \"heatmap\"}, {\"type\": \"heatmap\"}]\n",
    "    ],\n",
    "    subplot_titles=(\"0 - Original image\", \"1 - Scaled image\", \"2 - Contrasted image\", \"3 - Normalized image\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys'), row=1, col=1)\n",
    "image = scale(image)\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys'), row=1, col=2)\n",
    "image = contrast(image)\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys'), row=2, col=1)\n",
    "image = normalize(image)\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys'), row=2, col=2)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Segment Anything Model (SAM): Binary mask\n",
    "\n",
    "We initially attempted to predict all the classes for each image by assuming that each label corresponded to a distinct rock layer. In other words, we placed ourselves in an instance segmentation problem with as many classes as distinct labels, and we fine-tuned a Mask2former model, but this approach did not yield good results.\n",
    "\n",
    "Therefore, we pursued another approach. Since instance segmentation doesn't seem to produce results, we simplified the problem by converting the labels to 0 or 1 based on their parity. We transformed our problem into binary semantic segmentation. For this, we used a model from the [Segment Anything Model](https://arxiv.org/pdf/2304.02643.pdf) family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_label(label):\n",
    "    binary_label = np.where(label % 2 == 0, 1, 0)\n",
    "\n",
    "    return binary_label\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*3\n",
    "    ],\n",
    "    subplot_titles=(\"Processed image\", \"Original label\", \"Binarized label\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=1)\n",
    "image_label = label[slice_idx, :, :].T\n",
    "fig.add_trace(go.Heatmap(z=image_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "binary_label = get_binary_label(image_label.copy())\n",
    "fig.add_trace(go.Heatmap(z=binary_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a fake WandB run to make the code work.\n",
    "\n",
    "class RunDemo:\n",
    "    def __init__(self, config_file, id, name) -> None:\n",
    "        self.config = self.get_config(config_file)\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_config(config_file) -> dict:\n",
    "        root = os.path.join('config', config_file)\n",
    "        notebooks = os.path.join(os.pardir, root)\n",
    "        path = root if os.path.exists(root) else notebooks\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the SAM inference class to make predictions on the volume.\n",
    "\n",
    "sam_inference = sam_inf.SAMInference(\n",
    "    config=config,\n",
    "    cuda_idx=0,\n",
    "    list_volume=[volume_path],\n",
    "    run=None,\n",
    "    split='train',\n",
    "    batch=5 # Reduce the batch size if you encounter cuda memory issues running the inference. Min 2 Max 300\n",
    ")\n",
    "volume_name = os.path.basename(volume_path)\n",
    "binary_mask_path = sam_inference.get_mask_path(volume_name)\n",
    "sam_dir = os.path.split(binary_mask_path)[0]\n",
    "os.makedirs(sam_dir, exist_ok=True)\n",
    "sam_inference()\n",
    "binary_mask = np.load(binary_mask_path, allow_pickle=True)\n",
    "sam_binary_pred = binary_mask[slice_idx, :, :].T.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Deleting the instance to free up memory space.\n",
    "del sam_inference, binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*3\n",
    "    ],\n",
    "    subplot_titles=(\"Processed image\", \"Binarized label\", \"SAM Prediction\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=binary_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=sam_binary_pred.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used SAM without fine-tuning because it provided sufficient predictions for our pipeline.\n",
    "\n",
    "If you wish to create binary masks using SAM model from our solution, uncomment the following line and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_volume = md.get_volumes(config, 'test')\n",
    "sam_inference = sam_inf.SAMInference(\n",
    "    config=config,\n",
    "    cuda_idx=0,\n",
    "    list_volume=list_volume,\n",
    "    run=None,\n",
    "    split='test',\n",
    "    batch=5\n",
    ")\n",
    "\n",
    "sam_dir = sam_inference.get_folder_path()\n",
    "os.makedirs(sam_dir, exist_ok=True)\n",
    "sam_inference()\n",
    "\n",
    "del sam_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£1Ô∏è‚É£ Segformer: Binary mask\n",
    "\n",
    "As the binary mask generated by SAM was not of sufficiently good quality, we refined the prediction by using another model to generate the binary mask. For this, we used a model from the [Segformer](https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf) family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*3\n",
    "    ],\n",
    "    subplot_titles=(\"Red channel (Processed image)\", \"Green channel (SAM prediction)\", \"Blue channel (Processed image)\"),\n",
    ")\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=sam_binary_pred.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=3)\n",
    "fig.update_layout(showlegend=False, title='Decomposition of the Segformer input image into channels (RGB)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*2\n",
    "    ],\n",
    "    subplot_titles=(\"Original label\", \"Binarized label\")\n",
    ")\n",
    "\n",
    "image_label = label[slice_idx, :, :].T\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=1)\n",
    "binary_label = get_binary_label(image_label)\n",
    "fig.add_trace(go.Heatmap(z=binary_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Segformer inference class to make predictions on the volume.\n",
    "\n",
    "run = RunDemo('segformer.yml', id='mmw4795a', name='abundant-lantern-1231')\n",
    "\n",
    "segformer_inference = segformer_inf.SegformerInference(\n",
    "    config=config,\n",
    "    cuda_idx=0,\n",
    "    list_volume=[volume_path],\n",
    "    run=run,\n",
    "    split='train',\n",
    "    batch=150 # Reduce the batch size if you encounter issues running the inference. Min 2 Max 300\n",
    ")\n",
    "volume_name = os.path.basename(volume_path)\n",
    "binary_mask_path = segformer_inference.get_mask_path(volume_name)\n",
    "segformer_dir = os.path.split(binary_mask_path)[0]\n",
    "os.makedirs(segformer_dir, exist_ok=True)\n",
    "segformer_inference()\n",
    "binary_mask = np.load(binary_mask_path, allow_pickle=True)\n",
    "seg_binary_pred = binary_mask[slice_idx, :, :].T.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Deleting the instance to free up memory space.\n",
    "del segformer_inference, binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*3\n",
    "    ],\n",
    "    subplot_titles=(\"Processed image\", \"Binarized label\", \"Segformer Prediction\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=binary_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=seg_binary_pred.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, we achieved a dice score of `0.811` and an intersection over union (IOU) of `0.6857` on the binary masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to create binary masks using Segformer model from our solution, uncomment the following line and run the cell.\n",
    "\n",
    "You need to run the SAM inference before or change `data/processed/test/facebook_sam-vit-base-original` -> `data/processed/test/facebook_sam-vit-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = RunDemo('segformer.yml', id='mmw4795a', name='abundant-lantern-1231')\n",
    "list_volume = md.get_volumes(config, 'test')\n",
    "\n",
    "segformer_inference = segformer_inf.SegformerInference(\n",
    "    config=config,\n",
    "    cuda_idx=0,\n",
    "    list_volume=list_volume,\n",
    "    run=run,\n",
    "    split='test',\n",
    "    batch=5\n",
    ")\n",
    "\n",
    "segformer_dir = segformer_inference.get_folder_path()\n",
    "os.makedirs(segformer_dir, exist_ok=True)\n",
    "segformer_inference()\n",
    "\n",
    "del segformer_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Mask2former: Instance mask\n",
    "\n",
    "Once we have obtained our binary mask, we need to obtain each layer independently of its parity in order to create a prompt as precise as possible for the Segment Anything model.\n",
    "\n",
    "For this purpose, we will use another segmentation model called [Mask2former](http://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.pdf). This model will take as input the combination of the binary mask from Segformer and the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_label(label):\n",
    "    instance_label = np.full(label.shape, np.nan)\n",
    "    old_labels = np.unique(label)\n",
    "    new_labels = range(len(old_labels))\n",
    "    for old_label, new_label in zip(old_labels, new_labels):\n",
    "        instance_label = np.where(label == old_label, new_label, instance_label)\n",
    "\n",
    "    return instance_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*3\n",
    "    ],\n",
    "    subplot_titles=(\"Red channel (Seg prediction)\", \"Green channel (Original image)\", \"Blue channel (Seg prediction)\"),\n",
    ")\n",
    "fig.add_trace(go.Heatmap(z=seg_binary_pred.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=seg_binary_pred.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=3)\n",
    "fig.update_layout(showlegend=False, title='Decomposition of the Mask2former input image into channels (RGB)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its objective will be to predict a variant of the original label. The IDs of the original masks are set to 0 up to the number of masks present in the original label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label = label[slice_idx, :, :].T\n",
    "variant_label = get_instance_label(image_label)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*2\n",
    "    ],\n",
    "    subplot_titles=(\"Original label\", \"Variant label\"),\n",
    ")\n",
    "fig.add_trace(go.Heatmap(z=image_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=variant_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Mask2former inference class to make predictions on the volume.\n",
    "\n",
    "run = RunDemo('mask2former.yml', id='nvbtr9k2', name='vermilion-moon-1241')\n",
    "\n",
    "mask2former_inference = mask2former_inf.Mask2formerInference(\n",
    "    config=config,\n",
    "    cuda_idx=0,\n",
    "    list_volume=[volume_path],\n",
    "    run=run,\n",
    "    split='train',\n",
    "    batch=25 # Reduce the batch size if you encounter cuda memory issues running the inference. Min 2 Max 300\n",
    ")\n",
    "volume_name = os.path.basename(volume_path)\n",
    "instance_mask_path = mask2former_inference.get_mask_path(volume_name)\n",
    "mask2former_dir = os.path.split(instance_mask_path)[0]\n",
    "os.makedirs(mask2former_dir, exist_ok=True)\n",
    "\n",
    "mask2former_inference()\n",
    "instance_mask = np.load(instance_mask_path, allow_pickle=True)\n",
    "intance_pred = instance_mask[slice_idx, :, :].T\n",
    "\n",
    "del mask2former_inference, instance_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}]*3\n",
    "    ],\n",
    "    subplot_titles=(\"Original image\", \"Variant label\", \"Mask2former Prediction\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=image.tolist(), showscale=False, colorscale='Greys', hoverinfo='skip'), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=variant_label.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=intance_pred.tolist(), showscale=False, colorscale='viridis', hoverinfo='skip'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not want any class or background, we have chosen to set the weights of the focus loss and the cross-entropy loss to 1, while the dice loss has a weight of 10. Refer to [Hugging Face](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation).\n",
    "\n",
    "With this approach, we achieved a validation loss of `0.811` on the instance masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to create instance masks using Mask2former model from our solution, uncomment the following line and run the cell.\n",
    "\n",
    "You need to run the Segformer inference before or change `data/processed/test/abundant-lantern-1231-mmw4795a-original` -> `data/processed/test/abundant-lantern-1231-mmw4795a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = RunDemo('mask2former.yml', id='nvbtr9k2', name='vermilion-moon-1241')\n",
    "list_volume = md.get_volumes(config, 'test')\n",
    "\n",
    "mask2former_inference = mask2former_inf.Mask2formerInference(\n",
    "    config=config,\n",
    "    cuda_idx=0,\n",
    "    list_volume=list_volume,\n",
    "    run=run,\n",
    "    split='test',\n",
    "    batch=25\n",
    ")\n",
    "mask2former_dir = mask2former_inference.get_folder_path()\n",
    "os.makedirs(mask2former_dir, exist_ok=True)\n",
    "mask2former_inference()\n",
    "\n",
    "del mask2former_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßëüèª‚Äçüíª Code Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to change the configuration of the models, please refer to the YAML file available in the config folder.\n",
    "\n",
    "The script takes volumes from the data/raw/train folder for training and data/raw/test for inference. A directory data/processed/[train, test]/{run_id} is created for all intermediate masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Initialisation \n",
    "wandb_api_key = '02ca932e1203e93aaa8c97b8331d6c0b04c3170a' # Please do not share this api key\n",
    "! wandb login --relogin {wandb_api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "import src.models.segformer.train_model as segformer_tm\n",
    "import src.models.mask2former.train_model as mask2former_tm\n",
    "import src.features.segment_anything_inference as sam_inf\n",
    "import src.features.segformer_inference as segformer_inf\n",
    "import src.features.mask2former_inference as mask2former_inf\n",
    "import src.data.make_dataset as md\n",
    "import wandb\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Creating a fake WandB run to make the code work.\n",
    "\n",
    "class RunDemo:\n",
    "    def __init__(self, config_file, id, name) -> None:\n",
    "        self.config = self.get_config(config_file)\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_config(config_file) -> dict:\n",
    "        root = os.path.join('config', config_file)\n",
    "        notebooks = os.path.join(os.pardir, root)\n",
    "        path = root if os.path.exists(root) else notebooks\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        return config\n",
    "\n",
    "class SegmentationPipeline:\n",
    "    def __init__(self, sam_batch, segformer_batch, mask2former_batch) -> None:\n",
    "        self.config = utils.get_config()\n",
    "        self.sam_batch = sam_batch\n",
    "        self.segformer_batch = segformer_batch\n",
    "        self.mask2former_batch = mask2former_batch\n",
    "        \n",
    "    def make_sam_inference(self, split):\n",
    "        sam_inference = sam_inf.SAMInference(\n",
    "            config=self.config,\n",
    "            cuda_idx=0,\n",
    "            list_volume=md.get_volumes(self.config, split),\n",
    "            run=None,\n",
    "            split=split,\n",
    "            batch=self.sam_batch\n",
    "        )\n",
    "        os.makedirs(sam_inference.get_folder_path(), exist_ok=True)\n",
    "        sam_inference()\n",
    "        del sam_inference\n",
    "        \n",
    "    def make_segformer_inference(self, split, segformer_id):\n",
    "        segformer_inference = segformer_inf.SegformerInference(\n",
    "            config=self.config,\n",
    "            cuda_idx=0,\n",
    "            list_volume=md.get_volumes(self.config, split),\n",
    "            run=RunDemo('segformer.yml', **segformer_id),\n",
    "            split=split,\n",
    "            batch=self.sam_batch\n",
    "        )\n",
    "        os.makedirs(segformer_inference.get_folder_path(), exist_ok=True)\n",
    "        segformer_inference()\n",
    "        del segformer_inference\n",
    "        \n",
    "    def make_mask2former_inference(self, split, mask2former_id):\n",
    "        mask2former_inference = mask2former_inf.Mask2formerInference(\n",
    "            config=self.config,\n",
    "            cuda_idx=0,\n",
    "            list_volume=md.get_volumes(self.config, split),\n",
    "            run=RunDemo('mask2former.yml', **mask2former_id),\n",
    "            split=split,\n",
    "            batch=self.sam_batch\n",
    "        )\n",
    "        os.makedirs(mask2former_inference.get_folder_path(), exist_ok=True)\n",
    "        mask2former_inference()\n",
    "        del mask2former_inference\n",
    "    \n",
    "    def train(self):\n",
    "        print('Create Segment Anything binary masks...')\n",
    "        self.make_sam_inference(split='train')\n",
    "        print('Segment Anything binary masks done!')\n",
    "        print('Train Segformer...')\n",
    "        segformer_id = self.train_segformer()\n",
    "        print('Segformer training finished!')\n",
    "        print('Create Segformer binary masks...')\n",
    "        self.make_segformer_inference(split='train', segformer_id=segformer_id)\n",
    "        print('Segformer binary masks done!')\n",
    "        print('Train Mask2former')\n",
    "        mask2former_id = self.train_mask2former(segformer_id)\n",
    "        print('Mask2former training finished!')\n",
    "        \n",
    "        return {'segformer_id': segformer_id, 'mask2former_id': mask2former_id}\n",
    "    \n",
    "    def train_segformer(self):\n",
    "        wandb_config = utils.init_wandb('segformer.yml')\n",
    "        segformer_id = {'name': wandb.run.name, 'id': wandb.run.id}\n",
    "        trainer = segformer_tm.get_trainer(self.config)\n",
    "        lightning = segformer_tm.get_lightning(self.config, wandb_config)\n",
    "        trainer.fit(model=lightning)\n",
    "        wandb.finish()\n",
    "        \n",
    "        return segformer_id\n",
    "    \n",
    "    def train_mask2former(self, segformer_id):\n",
    "        utils.init_wandb('segformer.yml')\n",
    "        wandb.config.update(segformer_id=f'{segformer_id[\"name\"]}-{segformer_id[\"id\"]}')\n",
    "        wandb_config = wandb.config\n",
    "        mask2former_id = {'name': wandb.run.name, 'id': wandb.run.id}\n",
    "        trainer = mask2former_tm.get_trainer(self.config)\n",
    "        lightning = mask2former_tm.get_lightning(self.config, wandb_config)\n",
    "        trainer.fit(model=lightning)\n",
    "        wandb.finish()\n",
    "        \n",
    "        return mask2former_id\n",
    "    \n",
    "    def predict(self, segformer_id, mask2former_id):\n",
    "        print('Create Segment Anything binary masks...')\n",
    "        self.make_sam_inference(split='test')\n",
    "        print('Segment Anything binary masks done!')\n",
    "        print('Create Segformer binary masks...')\n",
    "        self.make_segformer_inference(split='test', segformer_id=segformer_id)\n",
    "        print('Segformer binary masks done!')\n",
    "        print('Create Mask2former instance masks...')\n",
    "        self.make_mask2former_inference(split='test', mask2former_id=mask2former_id)\n",
    "        print('Mask2former instance masks done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size  Min 2 Max 300\n",
    "segmentation_pipeline = SegmentationPipeline(sam_batch=5, segformer_batch=150, mask2former_batch=25)\n",
    "segmentation_pipeline.train()\n",
    "# segmentation_pipeline.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flair-2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
